{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d92862b6-bcf2-4ab6-bc47-eaa07c2a015f",
   "metadata": {},
   "source": [
    "#Original Author: Jonathan Hudson\n",
    "#CPSC 501 F22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38265ac1-5c33-4324-aedc-155483f53ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-11 13:39:35.470430: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-11 13:39:35.573529: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-11 13:39:35.573543: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-11 13:39:35.595296: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-11 13:39:36.670773: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-11 13:39:36.671289: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-11 13:39:36.671296: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Get data--\n",
      "--Process data--\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "print(\"--Get data--\")\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(\"--Process data--\")\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b118e511-7d40-4d34-96e0-e403a020d356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Make model--\n",
      "--Fit model--\n",
      "Epoch 1/30\n",
      "1875/1875 - 2s - loss: 0.2360 - accuracy: 0.9289 - 2s/epoch - 1ms/step\n",
      "Epoch 2/30\n",
      "1875/1875 - 2s - loss: 0.1032 - accuracy: 0.9685 - 2s/epoch - 1ms/step\n",
      "Epoch 3/30\n",
      "1875/1875 - 2s - loss: 0.0775 - accuracy: 0.9766 - 2s/epoch - 1ms/step\n",
      "Epoch 4/30\n",
      "1875/1875 - 2s - loss: 0.0622 - accuracy: 0.9813 - 2s/epoch - 1ms/step\n",
      "Epoch 5/30\n",
      "1875/1875 - 2s - loss: 0.0495 - accuracy: 0.9842 - 2s/epoch - 1ms/step\n",
      "Epoch 6/30\n",
      "1875/1875 - 2s - loss: 0.0424 - accuracy: 0.9870 - 2s/epoch - 1ms/step\n",
      "Epoch 7/30\n",
      "1875/1875 - 2s - loss: 0.0395 - accuracy: 0.9877 - 2s/epoch - 1ms/step\n",
      "Epoch 8/30\n",
      "1875/1875 - 2s - loss: 0.0316 - accuracy: 0.9900 - 2s/epoch - 1ms/step\n",
      "Epoch 9/30\n",
      "1875/1875 - 2s - loss: 0.0293 - accuracy: 0.9907 - 2s/epoch - 1ms/step\n",
      "Epoch 10/30\n",
      "1875/1875 - 2s - loss: 0.0278 - accuracy: 0.9911 - 2s/epoch - 1ms/step\n",
      "Epoch 11/30\n",
      "1875/1875 - 2s - loss: 0.0237 - accuracy: 0.9926 - 2s/epoch - 1ms/step\n",
      "Epoch 12/30\n",
      "1875/1875 - 2s - loss: 0.0229 - accuracy: 0.9931 - 2s/epoch - 1ms/step\n",
      "Epoch 13/30\n",
      "1875/1875 - 2s - loss: 0.0208 - accuracy: 0.9936 - 2s/epoch - 1ms/step\n",
      "Epoch 14/30\n",
      "1875/1875 - 2s - loss: 0.0191 - accuracy: 0.9944 - 2s/epoch - 1ms/step\n",
      "Epoch 15/30\n",
      "1875/1875 - 2s - loss: 0.0193 - accuracy: 0.9944 - 2s/epoch - 1ms/step\n",
      "Epoch 16/30\n",
      "1875/1875 - 2s - loss: 0.0181 - accuracy: 0.9945 - 2s/epoch - 1ms/step\n",
      "Epoch 17/30\n",
      "1875/1875 - 2s - loss: 0.0183 - accuracy: 0.9945 - 2s/epoch - 1ms/step\n",
      "Epoch 18/30\n",
      "1875/1875 - 2s - loss: 0.0132 - accuracy: 0.9961 - 2s/epoch - 1ms/step\n",
      "Epoch 19/30\n",
      "1875/1875 - 2s - loss: 0.0153 - accuracy: 0.9953 - 2s/epoch - 1ms/step\n",
      "Epoch 20/30\n",
      "1875/1875 - 2s - loss: 0.0148 - accuracy: 0.9958 - 2s/epoch - 1ms/step\n",
      "Epoch 21/30\n",
      "1875/1875 - 2s - loss: 0.0141 - accuracy: 0.9956 - 2s/epoch - 1ms/step\n",
      "Epoch 22/30\n",
      "1875/1875 - 2s - loss: 0.0127 - accuracy: 0.9963 - 2s/epoch - 1ms/step\n",
      "Epoch 23/30\n",
      "1875/1875 - 2s - loss: 0.0151 - accuracy: 0.9959 - 2s/epoch - 1ms/step\n",
      "Epoch 24/30\n",
      "1875/1875 - 2s - loss: 0.0131 - accuracy: 0.9962 - 2s/epoch - 1ms/step\n",
      "Epoch 25/30\n",
      "1875/1875 - 2s - loss: 0.0130 - accuracy: 0.9964 - 2s/epoch - 1ms/step\n",
      "Epoch 26/30\n",
      "1875/1875 - 2s - loss: 0.0132 - accuracy: 0.9962 - 2s/epoch - 1ms/step\n",
      "Epoch 27/30\n",
      "1875/1875 - 2s - loss: 0.0123 - accuracy: 0.9967 - 2s/epoch - 1ms/step\n",
      "Epoch 28/30\n",
      "1875/1875 - 2s - loss: 0.0100 - accuracy: 0.9969 - 2s/epoch - 1ms/step\n",
      "Epoch 29/30\n",
      "1875/1875 - 2s - loss: 0.0116 - accuracy: 0.9968 - 2s/epoch - 1ms/step\n",
      "Epoch 30/30\n",
      "1875/1875 - 2s - loss: 0.0120 - accuracy: 0.9969 - 2s/epoch - 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f00b84cd6f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"--Make model--\")\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"--Fit model--\")\n",
    "model.fit(x_train, y_train, epochs=30, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0386efc2-843f-4805-8f5d-e25125e677dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Evaluate model--\n",
      "1875/1875 - 1s - loss: 0.0069 - accuracy: 0.9981 - 1s/epoch - 735us/step\n",
      "313/313 - 0s - loss: 0.1330 - accuracy: 0.9807 - 249ms/epoch - 796us/step\n",
      "Train / Test Accuracy: 99.8% / 98.1%\n"
     ]
    }
   ],
   "source": [
    "print(\"--Evaluate model--\")\n",
    "model_loss1, model_acc1 = model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = model.evaluate(x_test,  y_test, verbose=2)\n",
    "print(f\"Train / Test Accuracy: {model_acc1*100:.1f}% / {model_acc2*100:.1f}%\")\n",
    "model.save(\"mnist_98.1_accuracy\", save_format=\"h5\") #Completed model with Train / Test Accuracy: 99.8% / 98.1%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebc5f34",
   "metadata": {},
   "source": [
    "\n",
    "Report\n",
    "--\n",
    "Getting the accuracy to 98+% was much easier than I had initially expected with this dataset. The only modification to hyper parameters that I made, was that I added more Rectified linear unit activation (RELU) layers to my model, and I also increased the density of the layers. Within these models there tends to be a lot of data that needs to be normalized, and if there is nodes that represent probabilities, we dont want these values to be non-positive. The RELU collapses all these negative values to 0, and I found that adding one got my accuracy to increase, and adding another got it to increase more. After adding a few more I was worried about overfitting, so I stopped adding them. But this did make the model get more out of the data that it was given, which is an improvement.\n",
    "Changes:\n",
    "```py\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "```\n",
    "\n",
    "When looking for other ways to improve the starter code, I noticed that the model was only trained for 1 epoch. I decided that this was less than ideal, and made it train for 30 epochs, which was still relatively quick. I found that after 30, there was quite a bit of diminishing returns in the amount of accuracy that it improved and the amount of time that it took for the model to reach new milestones in accuracy. When giving it higher epochs to train for, I found that this caused the model to overfit to the dataset.\n",
    "\n",
    "Another change that I made was that I changed the optimizer from SGD to Adam, because when looking through the documentation I found that Adam tends to have better computation time, and requires fewer hyper parameters for fine tuning.\n",
    "Changes:\n",
    "```py\n",
    "model.fit(x_train, y_train, epochs=30, verbose=2)\n",
    "```\n",
    "Overall, these three simple changes were more than enough to cause the model to blow past the 95% threshold that was required, and hit the 98% for bonus. I think the largest flaw with the model to begin with was that it wasnt normalizing the data that it had, and it also wasnt given enough time to make mistakes and improve on those mistakes with only 1 epoch. I think a basic model such as this, needs at least 30 epochs to be able to analyze data effectively with a large enough dataset. So by fixing those two mistakes, the model is in a much better position.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11982cf0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "d6d70f9a4759b97e421ee54bae746ca0ce0680c376ae7c2657ef0439157da5f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
