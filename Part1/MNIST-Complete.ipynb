{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d92862b6-bcf2-4ab6-bc47-eaa07c2a015f",
   "metadata": {},
   "source": [
    "#Original Author: Jonathan Hudson\n",
    "#CPSC 501 F22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38265ac1-5c33-4324-aedc-155483f53ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Get data--\n",
      "--Process data--\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "print(\"--Get data--\")\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(\"--Process data--\")\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b118e511-7d40-4d34-96e0-e403a020d356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Make model--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-08 16:12:14.909240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-08 16:12:14.911577: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-08 16:12:14.912920: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-08 16:12:14.914247: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-08 16:12:14.915450: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-11-08 16:12:14.916803: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-11-08 16:12:14.918145: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-08 16:12:14.919419: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-08 16:12:14.920751: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-11-08 16:12:14.920823: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-11-08 16:12:14.928801: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Fit model--\n",
      "Epoch 1/30\n",
      "1875/1875 - 2s - loss: 0.6623 - accuracy: 0.8230 - 2s/epoch - 1ms/step\n",
      "Epoch 2/30\n",
      "1875/1875 - 1s - loss: 0.3088 - accuracy: 0.9119 - 1s/epoch - 764us/step\n",
      "Epoch 3/30\n",
      "1875/1875 - 1s - loss: 0.2572 - accuracy: 0.9258 - 1s/epoch - 765us/step\n",
      "Epoch 4/30\n",
      "1875/1875 - 1s - loss: 0.2230 - accuracy: 0.9363 - 1s/epoch - 784us/step\n",
      "Epoch 5/30\n",
      "1875/1875 - 1s - loss: 0.1976 - accuracy: 0.9437 - 1s/epoch - 775us/step\n",
      "Epoch 6/30\n",
      "1875/1875 - 1s - loss: 0.1778 - accuracy: 0.9489 - 1s/epoch - 780us/step\n",
      "Epoch 7/30\n",
      "1875/1875 - 1s - loss: 0.1625 - accuracy: 0.9538 - 1s/epoch - 771us/step\n",
      "Epoch 8/30\n",
      "1875/1875 - 1s - loss: 0.1491 - accuracy: 0.9575 - 1s/epoch - 772us/step\n",
      "Epoch 9/30\n",
      "1875/1875 - 1s - loss: 0.1377 - accuracy: 0.9608 - 1s/epoch - 777us/step\n",
      "Epoch 10/30\n",
      "1875/1875 - 1s - loss: 0.1283 - accuracy: 0.9628 - 1s/epoch - 770us/step\n",
      "Epoch 11/30\n",
      "1875/1875 - 1s - loss: 0.1204 - accuracy: 0.9657 - 1s/epoch - 755us/step\n",
      "Epoch 12/30\n",
      "1875/1875 - 1s - loss: 0.1130 - accuracy: 0.9679 - 1s/epoch - 763us/step\n",
      "Epoch 13/30\n",
      "1875/1875 - 1s - loss: 0.1066 - accuracy: 0.9694 - 1s/epoch - 763us/step\n",
      "Epoch 14/30\n",
      "1875/1875 - 1s - loss: 0.1006 - accuracy: 0.9714 - 1s/epoch - 753us/step\n",
      "Epoch 15/30\n",
      "1875/1875 - 1s - loss: 0.0957 - accuracy: 0.9726 - 1s/epoch - 773us/step\n",
      "Epoch 16/30\n",
      "1875/1875 - 1s - loss: 0.0908 - accuracy: 0.9739 - 1s/epoch - 796us/step\n",
      "Epoch 17/30\n",
      "1875/1875 - 1s - loss: 0.0865 - accuracy: 0.9756 - 1s/epoch - 767us/step\n",
      "Epoch 18/30\n",
      "1875/1875 - 1s - loss: 0.0826 - accuracy: 0.9759 - 1s/epoch - 749us/step\n",
      "Epoch 19/30\n",
      "1875/1875 - 1s - loss: 0.0791 - accuracy: 0.9777 - 1s/epoch - 762us/step\n",
      "Epoch 20/30\n",
      "1875/1875 - 1s - loss: 0.0754 - accuracy: 0.9782 - 1s/epoch - 779us/step\n",
      "Epoch 21/30\n",
      "1875/1875 - 1s - loss: 0.0723 - accuracy: 0.9796 - 1s/epoch - 778us/step\n",
      "Epoch 22/30\n",
      "1875/1875 - 1s - loss: 0.0694 - accuracy: 0.9804 - 1s/epoch - 777us/step\n",
      "Epoch 23/30\n",
      "1875/1875 - 1s - loss: 0.0664 - accuracy: 0.9810 - 1s/epoch - 780us/step\n",
      "Epoch 24/30\n",
      "1875/1875 - 1s - loss: 0.0636 - accuracy: 0.9821 - 1s/epoch - 774us/step\n",
      "Epoch 25/30\n",
      "1875/1875 - 1s - loss: 0.0614 - accuracy: 0.9825 - 1s/epoch - 765us/step\n",
      "Epoch 26/30\n",
      "1875/1875 - 1s - loss: 0.0591 - accuracy: 0.9834 - 1s/epoch - 772us/step\n",
      "Epoch 27/30\n",
      "1875/1875 - 1s - loss: 0.0565 - accuracy: 0.9841 - 1s/epoch - 771us/step\n",
      "Epoch 28/30\n",
      "1875/1875 - 1s - loss: 0.0545 - accuracy: 0.9847 - 1s/epoch - 748us/step\n",
      "Epoch 29/30\n",
      "1875/1875 - 2s - loss: 0.0526 - accuracy: 0.9850 - 2s/epoch - 828us/step\n",
      "Epoch 30/30\n",
      "1875/1875 - 1s - loss: 0.0505 - accuracy: 0.9857 - 1s/epoch - 773us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f20e2f35180>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"--Make model--\")\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"--Fit model--\")\n",
    "model.fit(x_train, y_train, epochs=30, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0386efc2-843f-4805-8f5d-e25125e677dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Evaluate model--\n",
      "1875/1875 - 1s - loss: 0.0471 - accuracy: 0.9875 - 1s/epoch - 654us/step\n",
      "313/313 - 0s - loss: 0.0851 - accuracy: 0.9744 - 217ms/epoch - 694us/step\n",
      "Train / Test Accuracy: 98.7% / 97.4%\n"
     ]
    }
   ],
   "source": [
    "print(\"--Evaluate model--\")\n",
    "model_loss1, model_acc1 = model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = model.evaluate(x_test,  y_test, verbose=2)\n",
    "print(f\"Train / Test Accuracy: {model_acc1*100:.1f}% / {model_acc2*100:.1f}%\")\n",
    "model.save(\"model\", save_format=\"h5\") #Completed model with Train / Test Accuracy: 98.7% / 97.4%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebc5f34",
   "metadata": {},
   "source": [
    "\n",
    "Report\n",
    "--\n",
    "Getting the accuracy to ~97-98% was much easier than I had initially expected with this dataset. To only modification to hyper paramaters that I made, was that I added two Rectified linear unit activation (RELU) layers to my model. Within these models there tends to be a lot of data that needs to be normalized, and if there is nodes that represent probabilities, we dont want these values to be non-positive. The RELU collapses all these negative values to 0, and I found that adding one got my accuracy to increase, and adding another got it to increase more. Out of fear of over-fitting I did not add anymore. But this did make the model get more out of the data that it was given, which is an improvement.\n",
    "Changes:\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "When looking for other ways to improve the starter code, I noticed that the model was only trained for 1 epoch. I decided that this was less than ideal, and made it train for 30 epochs, which was still relatively quick. I found that after 30, there was quite a bit of diminishing returns in the amount of accuracy that it improved and the amount of time that it took for the model to reach new milestones in accuracy.\n",
    "Changes:\n",
    "model.fit(x_train, y_train, epochs=30, verbose=2)\n",
    "\n",
    "Overall, these two simple changes were more than enough to cause the model to blow past the 95% threshold that was required. I think the largest flaw with the model to begin with was that it wasnt normalizing the data that it had, and it also wasnt given enough time to make mistakes and improve on those mistakes with only 1 epoch. I think a basic model such as this, needs at least 10 epochs to be able to analyze data effectively with a large enough dataset. So by fixing those two mistakes, the model is in a much better position.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11982cf0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "d6d70f9a4759b97e421ee54bae746ca0ce0680c376ae7c2657ef0439157da5f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
